---
title: "EPARC Nested LASSO"
author: "Lucy Wang"
date: "12/24/2020"
output: 
  pdf_document:
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(ggplot2)
library(dplyr)
library(knitr)
library(reshape2)
library(GGally)
library(leaps)
library(caret)
library(xlsx)
library(gridExtra)
library(glmnet)

knitr::opts_chunk$set(fig.width = 8, message=F, 
                      echo=T, tidy.opts=list(width.cutoff=60), 
                      tidy=T)


setwd("/Users/lucywang/Google Drive/Doyle Lab/Final EPARC Epoxide")
```

Let's also define some useful functions
```{r functions}

# RMSE
rmse.f<-function(x, y){
  sqrt(mean((x-y)^2))
}

# Whenever we call something from a list in a function adn then 
# use sapply(), the columns of our dataframe are lists rather than vectors
# I'll run a simple function called unlist.f to help us unlist 
unlist.f<-function(df){
  #Number of columns in data frame 
  n<-ncol(df)
  for(i in 1:n){
    df[,i]<-unlist(df[,i])
  }
  return(df)
}
```

# Preprocessing 

## Reading in Data 

Read in df, Remove ee (as ΔΔG^‡^ are calculated from them) and ArCl, a character vector containing ligand names. For demonstration purposes, we are importing NiArCl dataset. 

```{r read_in}
df<-na.omit(read.csv("/Users/lucywang/Google Drive/Doyle Lab/Final EPARC Epoxide/[Export] Final Optimized PARC Epoxide NiArCl.csv"))

# Set aside ArCl names as a vector
ar<-df$ArCl

# Define coordination, a handy character vector for labeling graphs
coordination<-"LNiArCl"

# Delete some redundant variables 
df<-df[, -c(grep("ee",colnames(df)), 
           grep("ArCl",colnames(df)))]

#L.Class as factor variable
df$L.Class<-as.factor(df$L.Class)

# ID as character variable
df$ID<-as.character(df$ID)

# Change torsions to positive values 
df$Torsion<-abs(df$Torsion)
  
# Check structure 
str(df)

#Number of observations
m<-nrow(df)

# change output to ddG for simplicity 
colnames(df)[2]<-"ddG"

# set lambda value 
lambda="lambda.1se"
```

# The Outer Loop: Training-Validation and Test Split 

A repeated 4-fold (75-25) cross-validation, stratified split by ligand class (BiIM, non-benzyl BiOx, and benzyl BiOx) is conducted using `createMultiFolds` from the `caret` package. This 4-fold partition will be repeated 5 times. 

75% of the data will be the training-validation (tv) set, which will be used to select features for the multivariate linear regression model. The remaining 25% of the data will be our test set, which we will NOT subject to cross-validation or training; this set will be used once we've adequately trained our model with the training-validation set. 

We will also define some important variables: 
* `k.out`: number of folds to create for the outer loop (here, `k.out`=4)
* `l.out`: number of repeats to create for the outer loop (here, `l.out`=5)
* `n.out`: $k.out*l.out$, the total number of folds in the outer loop (here, `n.out`=20)

```{r outer_var}
# Define some pertinent variables for this outer loop 
k.out<-4
l.out<-5
  # total number of TV-Test Splits 
  n.out<-k.out*l.out
```

We will proceed with the splitting for the outer loop. `createMultiFolds()` outputs a list of indices for the training-validation set. 

* `fold_idx`: the list of indices that determine each fold 
* `df0`: the dataframe for splitting 

```{r split_trainvaltest}
# Set seed 
set.seed(1997, sample.kind="Rejection")

# Create Multiple folds index
folds.tvts<-createMultiFolds(df$L.Class, k=k.out, times=l.out)

# Write function that will take the indices of folds and split them 
# According to the indices in folds.tvts

split.tvts.f<-function(i, tv=TRUE){
  if(tv==TRUE){
  # print tv set 
  return(df[folds.tvts[[i]],]
  )
    }
  
# Otherwise, 
 # print test set 
 return(df[-folds.tvts[[i]],]
        )
}

```

For $i.out$ in $1,2,...,n.out$, we will use `split.out.f` to create the training-validation and test sets. We do this using `lapply()`, thus creating a list of `n.out` training-validation sets and a similar list of test sets. 

```{r save_tvtestsplit}
# Pass split.tvts.f through lapply
  # TV
  tv.l<-lapply(1:(k.out*l.out), 
       FUN=split.tvts.f, 
       tv=TRUE)

  # Test
  ts.l<-lapply(1:(k.out*l.out), 
       FUN=split.tvts.f, 
       tv=F)
```

Note that `tv.l[[i.out]]` $\cup$ `ts.l[[i]]` yields `df`.

We'll move on with feature scaling. 

##  Feature Scaling 

The values of each numerical feature will be standardized into a Z-score determined by the feature's mean and standard deviation. Only tv sets will be passed through the following function `scale.f`, which operates on only a single tv set.   

* `i.out`: defined above 
* `tv_list`: list of tv sets to pass through the function 
* `center`, `scale`, and `dfs` are logical argument; only one can be set to `TRUE` for a given call. 
    + `center`: outputs a list of feature means
    + `scale`: outputs a list of feature standard deviations
    + `dfs`: outputs the scaled features 
    
It's important to save `the means and standard deviations of the tv sets as they will be used later to scale the test features. 

```{r scale_fun}
mean_sc<-list()

scale.f<-function(i.out, tv_list,
                  center=FALSE, 
                  scale=FALSE, 
                  dfs=FALSE
                  ){
  # define x to be ith data frame in list 
  x<-tv_list[[i.out]]
  # Remove non-numerical variables 
  x.num<-x[,-c(
  grep("L.Class", colnames(x)), 
  grep("ID", colnames(x))
)]
  # Define character vector of column names 
  x.num.colnames<-colnames(x.num)
  
  # Scale and center 
  x.sc0<-scale(x.num)
  
    # Save mean 
    if(center==TRUE){
      return(attr(x.sc0, "scaled:center"))
    }

    # Save sd
    if(scale==TRUE){
      return(attr(x.sc0, "scaled:scale"))
    }
  
    # Save scaled tv matrix 
    if(dfs==TRUE){
      # Convert to data frame
      x.sc0<-as.data.frame(x.sc0)
      # add back colnames 
      colnames(x.sc0)<-x.num.colnames
      # add back ID and ligand class
      x.sc<-data.frame(ID=x$ID, 
                      x.sc0, 
                     L.Class=x$L.Class)
      # save 
      return(x.sc)
    }
}

```

Pass all `n.out` tv sets through `scale.f()` and save in list. Also save the means and standard deviations used for scaling. 

```{r scale_save}
# Concatenate scaled tv dfs 
tv.sc.l<-lapply(1:n.out, 
                FUN=scale.f, 
                tv_list=tv.l, 
                dfs=T)
                
# Concatenate feature means for each tv set 
tv.mean.l<-lapply(1:n.out, 
                FUN=scale.f, 
                tv_list=tv.l, 
                center=T)

# Concatenate feature sd for each tv set 
tv.sd.l<-lapply(1:n.out, 
                FUN=scale.f, 
                tv_list=tv.l, 
                scale=T)
```


We'll also need to scale our test set using scaling data from our tv set. 

### Scaling the Test Set 

We'll write a function to do this.`scale.ts.f()` scales each test set according to the scaling attributes (mean, standard deviation) of the corresponding tv set. 

* `test_list`: the list of unscaled test sets
* `center_list`: the list of means
* `scale_list`: the list of standard deviations

```{r scale_test}
scale.ts.f<-function(i.out, 
                     test_list, 
                     center_list,
                     scale_list){
  # Call test set 
  test<-test_list[[i.out]]
  
  # Remove ID, L.Class 
  test.num<-test[,-c(
    grep("ID", colnames(test)), 
    grep("L.Class", colnames(test))
  )]
  
  # Call center from list 
  center.i<-center_list[[i.out]]

  # Call scale from lsit 
  scale.i<-scale_list[[i.out]]
  
  # scale test.num
  
  test.sc0<-scale(test.num, 
                  center=center.i, 
                  scale=scale.i)
  
  # convert to df, add back ID and L.Class
  test.sc<-data.frame(
    "ID"=test$ID, 
    as.data.frame(test.sc0), 
    "L.Class"=test$L.Class
  )
  
  # return 
  return(test.sc)
}

#Apply our function 
ts.sc.l<-lapply(1:n.out, 
       FUN=scale.ts.f, 
       ts.l, 
       tv.mean.l,
       tv.sd.l)
```


We will now continue on with feature selection using LASSO.

# The Inner Loop: LASSO

## Protocol Overview 

`glmnet` already includes CV to search for $\lambda$,the L1 regularization penalty.For each tv-test split, we will use `glmnet` to perform LASSO; for modularity/ease of processing, we will write the `glmnet` operations within a function `lasso.f`. 

`glmnet` has a term $\alpha$, which modulates the regularization as ridge ($\alpha=0$), LASSO ($\alpha=1$), or elastic net ($0<\alpha<1$). We'll set $\alpha$ to 1 since we're interested in LASSO, which unlike ridge, can entirely "shrink" features completely to zero. This way, we can eliminate some features that are irrelevant or highly correlated with others.

  $Cost=\Sigma(y-\hat{y})^2 + \lambda \times [ \alpha (\Sigma |features|) + (1- \alpha) \Sigma features^2]$
  
Observe that when $\lambda=0$, the cost function reduces to OLS.

We will first split our training-validation and test data into input matrix and output ddG vector (y). 

## Split TV X, y

We'll define a function called `splitxy.f` to help us do this. `output` is a logical argument that determines whether we want to extract the input matrix or the output. 

```{r split_xy.f}
splitxy.f<-function(i.out, 
                    scaled_list, 
                    output=F){
  # call scaled tv set
  tv<-scaled_list[[i.out]]
  
  # input parameters with ID and L.Class removed 
    tv.x<-tv[,-c(
      grep("ID", colnames(tv)), 
      grep("ddG", colnames(tv)), 
      grep("L.Class", colnames(tv))
    )]
    
  # output vector
    tv.y<-tv$ddG
  
    # ifelse() to determine returned object depending on output arg
  ifelse(output==T, 
         return(tv.y), 
         return(tv.x)
  )
}
```

Apply this function. 

```{r split_tv_xy}
# create list of input parameter matrices for each scaled tv
tv.sc.x.l<-lapply(
  1:n.out, 
  FUN=splitxy.f, 
  tv.sc.l, 
)

# create list of output ddG vectors for each scaled tv 
tv.sc.y.l<-lapply(
  1:n.out, 
  FUN=splitxy.f, 
  tv.sc.l, 
  output=T
)

# create list of input parameter matrices for each scaled test set
ts.sc.x.l<-lapply(
  1:n.out, 
  FUN=splitxy.f, 
  ts.sc.l, 
)

# create list of output ddG vectors for each scaled test set
ts.sc.y.l<-lapply(
  1:n.out, 
  FUN=splitxy.f, 
  ts.sc.l, 
  output=T
)
```




Great! We'll now write our function that performs LASSO. 

## LASSO Function

We'll do this by writing `lasso.f`.  Within `cv.glmnet()`, we set `type.measure`, the argument for how the CV for $\lambda$ will be evaluated, to `mse` (mean squared error), and `family` to `gaussian` since we're doing linear regression. 

From cross-validation, $\lambda$ can be chosen two different ways (from `cv.glmnet()` documentation): 
1. Value of $\lambda$ that gives minimum cross-validated error. 
2. Largest value of $\lambda$ such that error is within 1 standard error of the $\lambda$ that had the smallest sum. 
    + Recall that as $\lambda$ increases, slope of the regression line approaches zero and more parameters are shrunk to zero. This will result in a simpler model. 
    
We can write this information into an argument called `lmd` that takes on either `lambda.min` or `lambda.1se`. In a previous code chunk, we had set `lambda` equal to "lambda.1se". 


```{r lasso_f, warning=F}
lasso.f<-function(i.out,
                  scaled_tv_x=tv.sc.x.l, 
                  scaled_tv_y=tv.sc.y.l, 
                  scaled_test_x=ts.sc.x.l, 
                  scaled_test_y=ts.sc.y.l,
                  scaled_tv=tv.sc.l,
                  scaled_test=ts.sc.l,
                  mean_list=tv.mean.l, 
                  scale_list=tv.sd.l,
                  a=1, 
                  lmd=lambda, output="ddG"){
# call input matrix x (tv)
  x.train<-as.matrix(scaled_tv_x[[i.out]])
  
# call output ddG vector y (tv)
  y.train<-scaled_tv_y[[i.out]]
  
# call scaled within test set 
  train<-scaled_tv[[i.out]]
  
# call input matrix (test)
   x.test<-as.matrix(scaled_test_x[[i.out]])
   
# call output ddG vector y (test)
   y.test<-scaled_test_y[[i.out]]

# call scaled  within test set 
  test<-scaled_test[[i.out]]
  
# call mean and sd of ddG for rescaling 
  mean<-mean_list[[i.out]][output]
  sd<-scale_list[[i.out]][output]
  
  
# fit linear regression with LASSO penalty 
  # using 10-fold CV to find value of lambda 
  
  alpha1.fit<-cv.glmnet(x.train, 
                        y.train, 
                        type.measure="mse", 
                        alpha=a, 
                        family="gaussian"
                        )
  
# Predict on test data 
  alpha1.predict.test<-predict(alpha1.fit, 
                            s=eval(parse(
                            text=paste('alpha1.fit$', lmd, 
                                       sep=""))), 
                          newx=x.test)
  
  # Predict training data 
  alpha1.predict.train<-predict(alpha1.fit, 
                          s=eval(parse(
                            text=
                              paste('alpha1.fit$', lmd, 
                                       sep="")
                              )
                            ),
                          newx=x.train)
  
# Make data frame comparing predicted to actual for test 
  df.pred.test<-data.frame(
    "ID"=test$ID, 
    "Predicted"=as.numeric(alpha1.predict.test), 
    "Measured"=y.test, 
    "Predicted Rescaled"=as.numeric(alpha1.predict.test)*sd+mean, 
    "Measured Rescaled"=y.test*sd+mean, 
    "L.Class"=test$L.Class
  )

  
# Make data frame comparing predicted to actual 
  df.pred.train<-data.frame(
    "ID"=train$ID, 
    "Predicted"= as.numeric(alpha1.predict.train), 
    "Measured"=y.train, 
    "Predicted Rescaled"=as.numeric(alpha1.predict.train)*sd+mean, 
    "Measured Rescaled"=y.train*sd+mean, 
    "L.Class"=train$L.Class
  )
  
# Finally, return RMSE values 
rmse<-c(rmse.f(df.pred.test[,2], 
               df.pred.test[,3]), 
        rmse.f(df.pred.train[,2], 
               df.pred.train[,3]), 
        rmse.f(df.pred.test[,4], 
               df.pred.test[,5]), 
        rmse.f(df.pred.train[,4], 
               df.pred.train[,5])
)

  names(rmse)<-c("Test Unscaled", 
                 "Train Unscaled", 
                 "Test Rescaled", 
                 "Train Rescaled")
  
return(list(
  'df.pred'=data.frame(rbind(df.pred.test, 
             df.pred.train), 
             "Set"=c(
               rep("Test", nrow(df.pred.test)), 
               rep("Train", nrow(df.pred.train))
             )
  ),
  'rmse'=rmse, 
  'lasso.fit'=alpha1.fit
))
  
}

```

We can now apply our function over `n.out` tv-test splits. Note we set $\lambda$ to `lambda.1se`. 

```{r apply_lasso.f, warning=F}
lasso.l<-lapply(1:n.out, 
                FUN=lasso.f)
```

## Finding Non-Zero Features

We'll now define a function `find.ft.f` that will take the results of the LASSO regularization and return the non-zero features and their coefficients. `lmd` is defined similarly. 

```{r find_ft_f}
find.ft.f<-function(i.out,
  lasso_list=lasso.l, 
  lmd=lambda){
  # Call lasso results of ith tv-test split 
  ls=lasso_list[[i.out]]
  
  # define result of cv.glmnet()
  fit=ls$lasso.fit
  
  # save coefficient matrix with lambda defined as lmd
  coef0<-as.matrix(
    coef(
      fit, 
      eval(parse(text=paste('fit$', lmd, 
                                       sep="")))
    )
  )
  
  # convert to data frame 
  df.coef0<-data.frame(
    "Feature"=rownames(coef0), 
    "Coefficient"=coef0[,1]
  )
  
  # omit features with coefficients of zero 
  
  df.coef<-df.coef0[df.coef0$Coefficient!=0,]
  
# Okay, we'll also create a "formula" string for future plotting purposes 
  f.string0<-df.coef$Feature
    # Remove intercept 
    f.string1<-f.string0[-grep(
      "(Intercept)", f.string0
    )]
    
    # Concatenate as single string
    f.string<-paste(f.string1, collapse="+")
    
      # further concatenate into formula by adding ddG~
      f.string.f<-paste(
        c("ddG~", f.string), 
        collapse=""
      )
    
      # if all features are shrunk, paste ddG~Intercept 
      if(length(f.string1)==0){
        f.string.f<-"ddG~Intercept"
      }
    
  return(list(
    "features"=df.coef, 
    "formula"=f.string.f
  )
)
}

```

We'll apply this function over all our tv-test splits. 

```{r apply_find_ft_f}
lasso.ft.l<-lapply(1:n.out, 
                   FUN=find.ft.f)
```

## Summarizing Features 

We have our LASSO predicted vs measured dfs for both tv/test data, the RMSEs for both tv/test data, and the regularized features after LASSO has shrunk some of our parameters to zero. We can start with a tally of which features were preserved and the test/tv rmses. 

As always, let's make a function for this. We'll call it `summ.lasso.f`. 

```{r summ_lasso_f}
summ.lasso.f<-function(i.out, 
                       lasso_output_list=lasso.l, 
                       lasso_ft_list=lasso.ft.l){
  # call output
    out<-lasso_output_list[[i.out]]
    
    # Call rmse 
      rmse<-out$rmse 
    
  # call features
    ft<-lasso_ft_list[[i.out]]
    
      # Call formula 
      form<-ft$formula 
    
  # Make a data frame row 
  
    df<-data.frame(
      "TV-Test Split"=i.out, 
      "Features"=form, 
      "Rescaled Train RMSE"=rmse[4],
      "Rescaled Test RMSE"=rmse[3]
    )
      # Remove any row names
    rownames(df)<-c()
    
  return(df)
}
```

We'll use `sapply()` to apply this function over all tv-test splits to return a dataframe.

```{r apply_summ_lasso_f}
# find lambda 1se
lmd<-rep(n.out)
  
  for(i in 1:n.out){
  lmd[i]<- eval(parse(text=paste('lasso.l[[i]]$lasso.fit$', lambda, 
                                       sep="")))
}

# create df as mentioned above
df.lasso<-data.frame(t(sapply(1:n.out, 
       summ.lasso.f)), 
       lmd)
  

  # use unlist.f
  df.lasso<-unlist.f(df.lasso)

    # melt for ggplot 
    df.lasso.summ.m<-melt(df.lasso, 
                          id.vars=c('Features', 
                                    'TV.Test.Split')
    )
      colnames(df.lasso.summ.m)[3]<-"RMSE"
```


```{r visualizing_lasso, fig.height=10, fig.width=7.5}
# Make boxplot of test and training RMSE 
# for each model
ggplot(df.lasso.summ.m, 
       aes(x=Features, 
           color=RMSE)
       )+ 
  geom_boxplot(aes(y=value)) + 
  geom_text(stat="count",
            aes(label=..count..), 
            y=0) + 
  facet_grid(rows=vars(RMSE)) + 
  theme(axis.text.x = element_text(angle=-45, 
                                   hjust=0,
                                   vjust=1)) + 
  ylab("RMSE") + 
  ylim(0, max(df.lasso.summ.m$value)
  ) + 
  ggtitle(paste(
    c("LASSO Test and Training RMSE,", coordination),
    collapse=" "
  ))
```

`df.lasso` can be summarized by examining average performance metrics and frequency of each unique feature set: 

```{r lasso_averages}
# use tapply() to calculate average tv/test RMSE as well as table()
  # the counts of each model
  
df.lasso.avg<-data.frame(
  "Model"=levels(as.factor(df.lasso$Features)), 
  "Avg Train RMSE"=as.numeric(
    tapply(df.lasso$Rescaled.Train.RMSE, 
                           df.lasso$Features,
                           mean)
                           ),
  "Avg Test RMSE"=as.numeric(
    tapply(df.lasso$Rescaled.Test.RMSE, 
                           df.lasso$Features,
                           mean)),
  lambda= as.numeric(
    tapply(df.lasso$lmd, 
                           df.lasso$Features,
                           mean)),
  "Count"=as.numeric(
    table(df.lasso$Features)
  )
)

# Reorder in decreasing frequnecy 
df.lasso.avg<-df.lasso.avg[
  order(-df.lasso.avg$Count),
]

# display 
kable(
  df.lasso.avg[order(
  df.lasso.avg$Count, decreasing=T
),])


coef(lasso.l[[6]]$lasso.fit)
```

