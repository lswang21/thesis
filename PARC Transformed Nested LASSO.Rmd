---
title: "NiArCl: Final Optimized PARC-LASSO"
author: "Lucy Wang"
date: "12/24/2020"
output: 
  pdf_document:
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(ggplot2)
library(dplyr)
library(knitr)
library(reshape2)
library(caret)
library(glmnet)

knitr::opts_chunk$set(fig.width = 8, message=F, 
                      echo=T, tidy.opts=list(width.cutoff=60), 
                      tidy=T)


setwd("/Users/lucywang/Google Drive/Doyle Lab/PARC 2/PARC Modeling")
```

# Preprocessing 

## Reading in Data 

Read in df, Define function `prep.f` that will preprocess the data, returning the processed dataframe 

`prep.f` takes the following arguments 
* `data`: the raw dataframe 
* `output`: a character string of output; here, we're interested in Yield, but can also change to another output like $\Delta \Delta G ^ {‡}$
* `transform`: a logical argument; if `transform=T`, applies log transformation on output 

```{r read_in}
df0<-na.omit(read.csv("Parc Ni Sty.csv"))
  epx="Styrene Oxides"

prep.f<-function(data, 
                 output="Yield", transform=F){
#change η to dent for convenience 
colnames(data)[which(colnames(data)=="η")]<-"dent"

#Class and denticity as factor variable
data$Class<-as.factor(data$Class)
data$dent<-as.factor(data$dent)

# ID as character variable
data$ID<-as.character(data$ID)

#Delete ddG output and denticity

ifelse(output=="Yield", 
       data<-data[,-c(
  grep("ddG", colnames(data)), 
  grep("dent", colnames(data))
)], 
data<-data[,-c(
  grep("dent", colnames(data)), 
  grep("Yield", colnames(data))
)])

# Transform

if(transform==T){
  data[,grep(output, colnames(data))]<-log(data[,grep(output, colnames(data))])
  
  # rename column
  colnames(data)[grep(output, colnames(data))]<-paste(c("ln", output), 
                                                                 collapse="")
}

return(data)
}

# Process data 
df<-prep.f(df0, transform=T)

# Define output as character string 
dv<-colnames(df)[3]

# define lambda for later use 
lambda="lambda.1se"
```

# The Outer Loop: Training-Validation and Test Split 

A repeated 4-fold (75-25) cross-validation, stratified split by ligand class (BiIM, non-benzyl BiOx, and benzyl BiOx) is conducted using `createMultiFolds` from the `caret` package. This 4-fold partition will be repeated 5 times. 

75% of the data will be the training-validation (tv) set, which will be used to select features for the multivariate linear regression model. The remaining 25% of the data will be our test set, which we will NOT subject to cross-validation or training; this set will be used once we've adequately trained our model with the training-validation set. 

We will also define some important variables: 
* `k.out`: number of folds to create for the outer loop (here, `k.out`=4)
* `l.out`: number of repeats to create for the outer loop (here, `l.out`=5)
* `n.out`: $k.out*l.out$, the totaly number of folds in the outer loop (here, `n.out`=20)

```{r outer_var}
k.out<-4
l.out<-5
n.out<-k.out*l.out
```

We will proceed with the splitting for the outer loop. `createMultiFolds()` outputs a list of indices for the training-validation set. 

* `fold_idx`: the list of indices that determine each fold 
* `df0`: the dataframe for splitting 

```{r outer_split}
# Set seed 
set.seed(1997, sample.kind="Rejection")

# Create outer repeated cv fold index
out_fold_idx<-createMultiFolds(df$Class, k=k.out, times=l.out)

# Write function that will take the indices of folds and split them 
# According to the indices in out_fold_idx
# if the argument tv=T, function will return training-validation folds 
# else, test folds

split.out.f<-function(i.out, df0=df, fold_idx=out_fold_idx, tv=TRUE){
  if(tv==TRUE){
  # return tv set 
  return(df0[fold_idx[[i.out]],]
  )
    }
  
# Otherwise, 
 # print test set 
 return(df0[-fold_idx[[i.out]],]
        )
}

```

For $i.out$ in $1,2,...,n.out$, we will use `split.out.f` to create the training-validation and test sets. We do this using `lapply()`, thus creating a list of `n.out` training-validation sets and a similar list of test sets. 
```{r outer_split_save}

  # TV
  tv.l<-lapply(1:n.out, 
       FUN=split.out.f, 
       tv=T)

  # Test
  ts.l<-lapply(1:(k.out*l.out), 
       FUN=split.out.f, 
       tv=F)
```

Note that `tv.l[[i.out]]` $\cup$ `ts.l[[i]]` yields `df`. 

We'll move on with feature scaling. 

##  Feature Scaling 

The values of each numerical feature will be standardized into a Z-score determined by the feature's mean and standard deviation. Only tv sets will be passed through the following function `scale.f`, which operates on only a single tv set.   

* `i.out`: defined above 
* `tv_list`: list of tv sets to pass through the function 
* `center`, `scale`, and `dfs` are logical argument; only one can be set to `TRUE` for a given call. 
    + `center`: outputs a list of feature means
    + `scale`: outputs a list of feature standard deviations
    + `dfs`: outputs the scaled features 
    
It's important to save `the means and standard deviations of the tv sets as they will be used later to scale the test features. 

```{r scale_fun}
mean_sc<-list()

scale.f<-function(i, tv_list,
                  center=FALSE, 
                  scale=FALSE, 
                  dfs=FALSE
                  ){
  # define x to be ith data frame in list 
  x<-tv_list[[i]]
  # Remove non-numerical variables 
  x.num<-x[,-c(
  grep("Class", colnames(x)), 
  grep("ID", colnames(x))
)]
  # Define character vector of column names 
  x.num.colnames<-colnames(x.num)
  
  # Scale and center 
  x.sc0<-scale(x.num)
  
    # Save mean 
    if(center==TRUE){
      return(attr(x.sc0, "scaled:center"))
    }

    # Save sd
    if(scale==TRUE){
      return(attr(x.sc0, "scaled:scale"))
    }
  
    # Save scaled tv matrix 
    if(dfs==TRUE){
      # Convert to data frame
      x.sc0<-as.data.frame(x.sc0)
      # add back colnames 
      colnames(x.sc0)<-x.num.colnames
      # add back ID and ligand class
      x.sc<-data.frame(ID=x$ID, 
                      x.sc0, 
                     Class=x$Class)
      # save 
      return(x.sc)
    }
}

```

```{r scale_f}

scale.f<-function(i.out, tv_list,
                  center=FALSE, 
                  scale=FALSE, 
                  dfs=FALSE
                  ){
  
  # define x to be ith data frame in list 
  x<-tv_list[[i.out]]
  
  # Because scale() only takes numerical data, 
  #remove non-numerical feature Ligand class and ID
  #and call the resulting numerical data frame x.num
  
  x.num<-x[,-c(
  grep("Class", colnames(x)), 
  grep("ID", colnames(x))
)]
  
  # Define character vector of column names 
  x.num.colnames<-colnames(x.num)
  
  # Scale and center 
  x.sc0<-scale(x.num)
  
    # Save mean 
    if(center==TRUE){
      return(attr(x.sc0, "scaled:center"))
    }

    # Save sd
    if(scale==TRUE){
      return(attr(x.sc0, "scaled:scale"))
    }
  
    # Save scaled tv matrix 
    
    if(dfs==TRUE){
      # Convert to data frame
      x.sc0<-as.data.frame(x.sc0)
      
      # add back colnames 
      colnames(x.sc0)<-x.num.colnames
      
      # add back ID and ligand class
      x.sc<-data.frame(ID=x$ID, 
                      x.sc0, 
                     Class=x$Class)
      # save 
      return(x.sc)
    }
}

```

Pass all `n.out` tv sets through `scale.f()` and save in list. Also save the means and standard deviations used for scaling. 
```{r scale_save}
# Concatenate scaled tv dfs 
tv.sc.l<-lapply(1:n.out, 
                FUN=scale.f, 
                tv_list=tv.l, 
                dfs=T)
                
# Concatenate feature means for each tv set 
tv.mean.l<-lapply(1:n.out, 
                FUN=scale.f, 
                tv_list=tv.l, 
                center=T)

# Concatenate feature sd for each tv set 
tv.sd.l<-lapply(1:n.out, 
                FUN=scale.f, 
                tv_list=tv.l, 
                scale=T)
```


We will now continue on with LASSO. 

### Scaling the Test Set 

We'll write a function to do this.`scale.ts.f()` scales each test set according to the scaling attributes (mean, standard deviation) of the corresponding tv set. 

* `test_list`: the list of unscaled test sets
* `center_list`: the list of means
* `scale_list`: the list of standard deviations

```{r scale_test}
scale.ts.f<-function(i.out, 
                     test_list, 
                     center_list,
                     scale_list){
  # Call test set 
  test<-test_list[[i.out]]
  
  # Remove ID, Class 
  test.num<-test[,-c(
    grep("ID", colnames(test)), 
    grep("Class", colnames(test))
  )]
  
  # Call center from list 
  center.i<-center_list[[i.out]]

  # Call scale from lsit 
  scale.i<-scale_list[[i.out]]
  
  # scale test.num
  
  test.sc0<-scale(test.num, 
                  center=center.i, 
                  scale=scale.i)
  
  # convert to df, add back ID and Class
  test.sc<-data.frame(
    "ID"=test$ID, 
    as.data.frame(test.sc0), 
    "Class"=test$Class
  )
  
  # return 
  return(test.sc)
}

# Apply our function 
ts.sc.l<-lapply(1:n.out, 
       FUN=scale.ts.f, 
       ts.l, 
       tv.mean.l,
       tv.sd.l)
```


We will now continue on with feature selection using LASSO.

# The Inner Loop: LASSO

## Protocol Overview 

`glmnet` already includes CV to search for $\lambda$,the L1 regularization penalty.For each tv-test split, we will use `glmnet` to perform LASSO; for modularity/ease of processing, we will write the `glmnet` operations within a function `lasso.f`. 

`glmnet` has a term $\alpha$, which modulates the regularization as ridge ($\alpha=0$), LASSO ($\alpha=1$), or elastic net ($0<\alpha<1$). We'll set $\alpha$ to 1 since we're interested in LASSO, which unlike ridge, can entirely "shrink" features completely to zero. This way, we can eliminate some features that are irrelevant or highly correlated with others.

  $Cost=\Sigma(y-\hat{y})^2 + \lambda \times [ \alpha (\Sigma |features|) + (1- \alpha) \Sigma features^2]$
  
Observe that when $\lambda=0$, the cost function reduces to OLS.

We will first split our training-validation and test data into input matrix and output lnYield vector (y). 

## Split TV X, y

We'll define a function called `splitxy.f` to help us do this. `output` is a logical argument that determines whether we want to extract the input matrix or the output. 

```{r split_xy.f}
splitxy.f<-function(i.out, 
                    scaled_list, 
                    output=F){
  # call scaled tv set
  tv<-scaled_list[[i.out]]
  
  # input parameters with ID and Class removed 
    tv.x<-tv[,-c(
      grep("ID", colnames(tv)), 
      grep(dv, colnames(tv)), 
      grep("Class", colnames(tv))
    )]
    
  # output vector
    tv.y<-tv$lnYield
  
    # ifelse() to determine returned object depending on output arg
  ifelse(output==T, 
         return(tv.y), 
         return(tv.x)
  )
}

# Also define rmse.f, which calculates rmse 
rmse.f<-function(x,y){
  return(sqrt(mean((x-y)^2)))
}

unlist.f<-function(x){
  for(i in 1:ncol(x)){
    x[,i]<-unlist(x[,i])
  }
  
  return(x)
}
```

Apply this function. 

```{r split_tv_xy}
# create list of input parameter matrices for each scaled tv
tv.sc.x.l<-lapply(
  1:n.out, 
  FUN=splitxy.f, 
  tv.sc.l, 
)

# create list of output lnYield vectors for each scaled tv 
tv.sc.y.l<-lapply(
  1:n.out, 
  FUN=splitxy.f, 
  tv.sc.l, 
  output=T
)

# create list of input parameter matrices for each scaled test set
ts.sc.x.l<-lapply(
  1:n.out, 
  FUN=splitxy.f, 
  ts.sc.l, 
)

# create list of output lnYield vectors for each scaled test set
ts.sc.y.l<-lapply(
  1:n.out, 
  FUN=splitxy.f, 
  ts.sc.l, 
  output=T
)
```


Great! We'll now write our function that performs LASSO. 

## LASSO Function

We'll do this by writing `lasso.f`.  Within `cv.glmnet()`, we set `type.measure`, the argument for how the CV for $\lambda$ will be evaluated, to `mse` (mean squared error), and `family` to `gaussian` since we're doing linear regression. 

From cross-validation, $\lambda$ can be chosen two different ways (from `cv.glmnet()` documentation): 
1. Value of $\lambda$ that gives minimum cross-validated error. 
2. Largest value of $\lambda$ such that error is within 1 standard error of the $\lambda$ that had the smallest sum. 
    + Recall that as $\lambda$ increases, slope of the regression line approaches zero and more parameters are shrunk to zero. This will result in a simpler model. 
    
We can write this information into an argument called `lmd` that takes on either `lambda.min` or `lambda.1se`. In a previous code chunk, we had set `lambda` equal to "lambda.1se". 

```{r lasso_f, warning=F}
lasso.f<-function(i.out,
                  scaled_tv_x=tv.sc.x.l, 
                  scaled_tv_y=tv.sc.y.l, 
                  scaled_test_x=ts.sc.x.l, 
                  scaled_test_y=ts.sc.y.l,
                  scaled_tv=tv.sc.l,
                  scaled_test=ts.sc.l,
                  a=1, 
                  lmd=lambda){
# call input matrix x (tv)
  x.train<-as.matrix(scaled_tv_x[[i.out]])
  
# call output lnYield vector y (tv)
  y.train<-scaled_tv_y[[i.out]]
  
# call scaled within test set 
  train<-scaled_tv[[i.out]]
  
# call input matrix (test)
   x.test<-as.matrix(scaled_test_x[[i.out]])
   
# call output lnYield vector y (test)
   y.test<-scaled_test_y[[i.out]]

# call scaled  within test set 
  test<-scaled_test[[i.out]]
  
# fit linear regression with LASSO penalty 
  # using 10-fold CV to find value of lambda 
  
  alpha1.fit<-cv.glmnet(x.train, 
                        y.train, 
                        type.measure="mse", 
                        alpha=a, 
                        family="gaussian"
                        )
  
# Predict on test data 
  alpha1.predict.test<-predict(alpha1.fit, 
                            s=eval(parse(
                            text=paste('alpha1.fit$', lmd, 
                                       sep=""))), 
                          newx=x.test)
  
# Make data frame comparing predicted to actual 
  df.pred.test<-data.frame(
    "ID"=test$ID, 
    "Predicted Transformed"=as.numeric(alpha1.predict.test), 
    "Measured Transformed"=y.test, 
    "Class"=test$Class
  )
  
# Predict training data 
  alpha1.predict.train<-predict(alpha1.fit, 
                          s=eval(parse(
                            text=
                              paste('alpha1.fit$', lmd, 
                                       sep="")
                              )
                            ),
                          newx=x.train)
  
# Make data frame comparing predicted to actual 
  df.pred.train<-data.frame(
    "ID"=train$ID, 
    "Predicted Transformed"= as.numeric(alpha1.predict.train), 
    "Measured Transformed"=y.train, 
    "Class"=train$Class
  )
  
# Finally, return RMSE values 
rmse<-c(rmse.f(df.pred.test[,2], 
               df.pred.test[,3]), 
        rmse.f(df.pred.train[,2], 
               df.pred.train[,3]))
  names(rmse)<-c("Test", 
                 "Train")
  
return(list(
  'df.pred'=data.frame(rbind(df.pred.test, 
             df.pred.train), 
             "Set"=c(
               rep("Test", nrow(df.pred.test)), 
               rep("Train", nrow(df.pred.train))
             )
  ),
  'rmse'=rmse, 
  'lasso.fit'=alpha1.fit
))
  
}

```

We can now apply our function over `n.out` tv-test splits. Note we set $\lambda$ to `lambda.1se`. 

```{r apply_lasso.f, warning=F}
lasso.l<-lapply(1:n.out, 
                FUN=lasso.f)
```

## Finding non-zero Features

We'll now define a function `find.ft.f` that will take the results of the LASSO regularization and return the non-zero features and their coefficients. `lmd` is defined similarly. 

```{r find_ft_f}
find.ft.f<-function(i.out,
  lasso_list=lasso.l, 
  lmd=lambda){
  # Call lasso results of ith tv-test split 
  ls=lasso_list[[i.out]]
  
  # define result of cv.glmnet()
  fit=ls$lasso.fit
  
  # save coefficient matrix with lambda defined as lmd
  coef0<-as.matrix(
    coef(
      fit, 
      eval(parse(text=paste('fit$', lmd, 
                                       sep="")))
    )
  )
  
  # convert to data frame 
  df.coef0<-data.frame(
    "Feature"=rownames(coef0), 
    "Coefficient"=coef0[,1]
  )
  
  # omit features with coefficients of zero 
  
  df.coef<-df.coef0[df.coef0$Coefficient!=0,]
  
# Okay, we'll also create a "formula" string for future plotting purposes 
  f.string0<-df.coef$Feature
    # Remove intercept 
    f.string1<-f.string0[-grep(
      "(Intercept)", f.string0
    )]
    
    # Concatenate as single string
    f.string<-paste(f.string1, collapse="+")
    
      # further concatenate into formula by adding lnYield~
      f.string.f<-paste(
        c("lnYield~", f.string), 
        collapse=""
      )
    
      # if all features are shrunk, paste lnYield~Intercept 
      if(length(f.string1)==0){
        f.string.f<-"lnYield~Intercept"
      }
    
  return(list(
    "features"=df.coef, 
    "formula"=f.string.f, 
    "coef"=df.coef
  )
)
}

```

We'll apply this function over all our tv-test splits. 

```{r apply_find_ft_f}
lasso.ft.l<-lapply(1:n.out, 
                   FUN=find.ft.f)
```

Backtransform predicted using Duan's smearing adjustment and measured rmse: 

$$y_i=\hat{y}_i+ \varepsilon $$
$$\hat{y}^*_i=e^{\hat{y}_i} \times \frac{1}{n} \sum_{i=1}^n e^{\varepsilon} $$

```{r backtransform}
bt.f<-function(i.out, lasso_list=lasso.l, 
               mean_list=tv.mean.l, scale_list=tv.sd.l, 
               output="lnYield"){
  # Call prediction and scalign attributes
  x<-lasso_list[[i.out]]$df.pred
  x.mean<-mean_list[[i.out]][output]
  x.sd<-scale_list[[i.out]][output]
  
  # Rescale predictions 
  x.pred.rs<-x$Predicted.Transformed*x.sd+x.mean
  x.meas.rs<-x$Measured.Transformed*x.sd+x.mean
  
  # Define number of test and length of entire df
  n.ts<-nrow(x[x$Set=="Test",])
  n<-nrow(x)
  
  # Find two residuals: test and tv 
  res.ts<-(x.meas.rs-x.pred.rs)[1:n.ts]
  res.tv<-(x.meas.rs-x.pred.rs)[(n.ts+1):n]
  
  # raise e to measured lnYield to return to yield space 
  x.meas.bt<-exp(x.meas.rs)
  
  # duan's smearing adjustment 
  x.pred.bt<-c(exp(x.pred.rs[1:n.ts])*mean(exp(res.ts)),
               exp(x.pred.rs[(n.ts+1):n])*mean(exp(res.tv))
  )
  
  # Create 
  lasso_list[[i.out]]$df.pred$"BT.Measured"<-x.meas.bt
  lasso_list[[i.out]]$df.pred$"BT.Predicted"<-x.pred.bt
  
 return(lasso_list[[i.out]])
}

bt.pred.l<-lapply(1:n.out, FUN=bt.f)
```

We'll now find back-transformed RMSEs. 

```{r}
rmse.bt.f<-function(i.out, bt_list=bt.pred.l){
  # Call df 
  x<-bt_list[[i.out]]$df.pred
  
  # Split into test adn tv 
  tv<-x[x$Set=="Train",]
  ts<-x[x$Set=="Test",]
 
  # Find RMSE 
  tv.rmse<-sqrt(mean((tv$BT.Measured-tv$BT.Predicted)^2))
  ts.rmse<-sqrt(mean((ts$BT.Measured-ts$BT.Predicted)^2))
  
  # Concatenate into vector 
  return(c(i.out, tv.rmse, ts.rmse))
}


# Apply across all outer folds

df.bt.rmse<-as.data.frame(t(sapply(1:n.out, rmse.bt.f)))
  colnames(df.bt.rmse)<-c("i.out", 
                          "BT.TV.RMSE", 
                          "BT.Test.RMSE")
```

## Summarizing Features 

We have our LASSO predicted vs measured dfs for both tv/test data, the RMSEs for both tv/test data, and the regularized features after LASSO has shrunk some of our parameters to zero. We can start with a tally of which features were preserved and the test/tv rmses.

Before we do this, we'll create a for loop that will extract $\lambda$s. 

```{r lambda}
# create empty matrix to store lambdas
lmd<-matrix(nrow=n.out, ncol=2)

# for loop to find lambda 
  for(i in 1:n.out){
    lmd[i,]<-c(lasso.l[[i]]$lasso.fit$lambda.1se, 
               lasso.l[[i]]$lasso.fit$lambda.min)
  }

# convert to data frame 
lmd<-as.data.frame(lmd)

  # alter column name 
  colnames(lmd)<-c("lmd.1se", "lmd.min")
```

Let'summarize our LASSO results with the function `summ.lasso.f`. 

```{r summ_lasso_f}
summ.lasso.f<-function(i.out, 
                       lasso_ft_list=lasso.ft.l){
    
  # call features
    ft<-lasso_ft_list[[i.out]]
    
      # Call formula 
      form<-ft$formula 
  return(form)
}
```

We'll use `sapply()` to apply this function over all tv-test splits to return a dataframe.


```{r apply_summ_lasso_f}
# create df as mentioned above
df.lasso<-data.frame(Features=(sapply(1:n.out, 
       summ.lasso.f)
  ), 
  df.bt.rmse, 
  lmd
) 

    # melt for ggplot 
    df.lasso.m<-melt(df.lasso, 
                          id.vars=c('Features', 
                                    'i.out', 'lmd.1se', 
                                    'lmd.min')
    )
      colnames(df.lasso.m)[5]<-"Back_Transformed_RMSE"
      
      # Features as factor
      df.lasso.m$Features<-as.factor(df.lasso.m$Features)
      
      # or plotting ease, replace models with numbers 
      levels(df.lasso.m$Features)<-1:length(levels(df.lasso.m$Features))

```

Plot the distribution of TV and test RMSEs. 

```{r visualizing_lasso, fig.height=10, fig.width=7.5}
# Make boxplot of test and training RMSE 
# for each model
ggplot(df.lasso.m, 
       aes(x=Features, y=value,
           color=Back_Transformed_RMSE)
       )+ 
  geom_boxplot(aes(y=value)) + 
  geom_text(stat="count",
            aes(label=..count..), 
            y=0) + 
  facet_grid(rows=vars(Back_Transformed_RMSE), 
             scales='free') + 
  theme(axis.text.y = element_text(angle=45, 
                                   hjust=0,
                                   vjust=1)) + 
  ylab("Back-Transformed RMSE") + 
  ylim(0, max(df.lasso.m$value)
  ) + 
  ggtitle(paste(
    c("LASSO Test and Training RMSE,", epx),
    collapse=" "
  )) + 
  coord_flip()
```


Concatenate `df.lasso` to display counts average performance metrics. 
```{r lasso_averages}
# use tapply() to calculate average tv/test RMSE as well as table()
  # the counts of each model
  
df.lasso.avg<-data.frame(
  "Model"=levels(as.factor(df.lasso$Features)), 
  "Avg BT Train RMSE"=as.numeric(
    tapply(df.lasso$BT.TV.RMSE, 
                           df.lasso$Features,
                           mean)
                           ),
  "Avg Test RMSE"=as.numeric(
    tapply(df.lasso$BT.Test.RMSE, 
                           df.lasso$Features,
                           mean)),
  
  "Avg lmd.1se"=as.numeric(
    tapply(df.lasso$lmd.1se, df.lasso$Features, mean)
  ),
  
  "Count"=as.numeric(
    table(df.lasso$Features)
  )
)

# display 
kable(
  df.lasso.avg[order(
  df.lasso.avg$Count, decreasing=T
),])
```

