---
title: "EPARC Nested CV OLS"
author: "Lucy Wang"
date: "12/24/2020"
output: 
  pdf_document:
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(ggplot2)
library(knitr)
library(reshape2)
library(leaps)
library(caret)
library(xlsx)
library(gridExtra)



knitr::opts_chunk$set(fig.width = 8, message=F, 
                      echo=T, tidy.opts=list(width.cutoff=60), 
                      tidy=T)


setwd("/Users/lucywang/Google Drive/Doyle Lab/Final EPARC Epoxide")
```

# Preprocessing 

## Reading in Data 

Read in df, Remove ee (as ΔΔG^‡^ are calculated from them) and ArCl, a character vector containing ligand names. For demonstration purposes, we are importing NiArCl dataset. 

```{r read_in}
df<-na.omit(read.csv("/Users/lucywang/Google Drive/Doyle Lab/Final EPARC Epoxide/[Export] Final Optimized PARC Epoxide NiArCl.csv"))

# Set aside ArCl names as a vector
ar<-df$ArCl

# Define coordination, a handy character vector for labeling graphs
coordination<-"LNiArCl"

# Delete some redundant variables 
df<-df[, -c(grep("ee",colnames(df)), 
           grep("ArCl",colnames(df)))]

#L.Class as factor variable
df$L.Class<-as.factor(df$L.Class)

# ID as character variable
df$ID<-as.character(df$ID)

# Change torsions to positive values 
df$Torsion<-abs(df$Torsion)
  
# Check structure 
str(df)

#Number of observations
m<-nrow(df)

# change output to ddG for simplicity 
colnames(df)[2]<-"ddG"
```

# The Outer Loop: Training-Validation and Test Split 

A repeated 4-fold (75-25) cross-validation, stratified split by ligand class (BiIM, non-benzyl BiOx, and benzyl BiOx) is conducted using `createMultiFolds` from the `caret` package. This 4-fold partition will be repeated 5 times. 

75% of the data will be the training-validation (tv) set, which will be used to select features for the multivariate linear regression model. The remaining 25% of the data will be our test set, which we will NOT subject to cross-validation or training; this set will be used once we've adequately trained our model with the training-validation set. 

We will also define some important variables: 
* `k.out`: number of folds to create for the outer loop (here, `k.out`=4)
* `l.out`: number of repeats to create for the outer loop (here, `l.out`=5)
* `n.out`: $k.out*l.out$, the totaly number of folds in the outer loop (here, `n.out`=20)

```{r outer_var}
k.out<-4
l.out<-5
n.out<-k.out*l.out
```

We will proceed with the splitting for the outer loop. `createMultiFolds()` outputs a list of indices for the training-validation set. 

* `fold_idx`: the list of indices that determine each fold 
* `df0`: the dataframe for splitting 

```{r outer_split}
# Set seed 
set.seed(1997, sample.kind="Rejection")

# Create outer repeated cv fold index
out_fold_idx<-createMultiFolds(df$L.Class, k=k.out, times=l.out)

# Write function that will take the indices of folds and split them 
# According to the indices in out_fold_idx
# if the argument tv=T, function will return training-validation folds 
# else, test folds

split.out.f<-function(i.out, df0=df, fold_idx=out_fold_idx, tv=TRUE){
  if(tv==TRUE){
  # return tv set 
  return(df0[fold_idx[[i.out]],]
  )
    }
  
# Otherwise, 
 # print test set 
 return(df0[-fold_idx[[i.out]],]
        )
}

```

For $i.out$ in $1,2,...,n.out$, we will use `split.out.f` to create the training-validation and test sets. We do this using `lapply()`, thus creating a list of `n.out` training-validation sets and a similar list of test sets. 
```{r outer_split_save}

  # TV
  tv.l<-lapply(1:n.out, 
       FUN=split.out.f, 
       tv=T)

  # Test
  ts.l<-lapply(1:(k.out*l.out), 
       FUN=split.out.f, 
       tv=F)
```

Note that `tv.l[[i.out]]` $\cup$ `ts.l[[i]]` yields `df`. 

We'll move on with feature scaling. 

##  Feature Scaling 

The values of each numerical feature will be standardized into a Z-score determined by the feature's mean and standard deviation. Only tv sets will be passed through the following function `scale.f`, which operates on only a single tv set.   

* `i.out`: defined above 
* `tv_list`: list of tv sets to pass through the function 
* `center`, `scale`, and `dfs` are logical argument; only one can be set to `TRUE` for a given call. 
    + `center`: outputs a list of feature means
    + `scale`: outputs a list of feature standard deviations
    + `dfs`: outputs the scaled features 
    
It's important to save `the means and standard deviations of the tv sets as they will be used later to scale the test features. 
    

```{r scale_f}

scale.f<-function(i.out, tv_list,
                  center=FALSE, 
                  scale=FALSE, 
                  dfs=FALSE
                  ){
  
  # define x to be ith data frame in list 
  x<-tv_list[[i.out]]
  
  # Because scale() only takes numerical data, 
  #remove non-numerical feature Ligand class and ID
  #and call the resulting numerical data frame x.num
  
  x.num<-x[,-c(
  grep("L.Class", colnames(x)), 
  grep("ID", colnames(x))
)]
  
  # Define character vector of column names 
  x.num.colnames<-colnames(x.num)
  
  # Scale and center 
  x.sc0<-scale(x.num)
  
    # Save mean 
    if(center==TRUE){
      return(attr(x.sc0, "scaled:center"))
    }

    # Save sd
    if(scale==TRUE){
      return(attr(x.sc0, "scaled:scale"))
    }
  
    # Save scaled tv matrix 
    
    if(dfs==TRUE){
      # Convert to data frame
      x.sc0<-as.data.frame(x.sc0)
      
      # add back colnames 
      colnames(x.sc0)<-x.num.colnames
      
      # add back ID and ligand class
      x.sc<-data.frame(ID=x$ID, 
                      x.sc0, 
                     L.Class=x$L.Class)
      # save 
      return(x.sc)
    }
}

```

Pass all `n.out` tv sets through `scale.f()` and save in list. Also save the means and standard deviations used for scaling. 
```{r scale_save}
# Concatenate scaled tv dfs 
tv.sc.l<-lapply(1:n.out, 
                FUN=scale.f, 
                tv_list=tv.l, 
                dfs=T)
                
# Concatenate feature means for each tv set 
tv.mean.l<-lapply(1:n.out, 
                FUN=scale.f, 
                tv_list=tv.l, 
                center=T)

# Concatenate feature sd for each tv set 
tv.sd.l<-lapply(1:n.out, 
                FUN=scale.f, 
                tv_list=tv.l, 
                scale=T)
```


We will now continue on with feature selection. 

# The Inner Loop: Feature Selection 

## Protocol Overview 
The protocol is thus: 

1. The $i_{out}^{th}$ training-validation validtion will be subjected to a $k_{in}$-fold (here, 5) cross validation, repeated $l_{in}=10$ times, generating $k_{in}*l_{in}$ (here, 50) validation sets. 
2. Each $k_{in}^{th}$ fold of the $l_{in}^{th}$ repeat will serve as the validation set, and the remaining $k_{in}-1$ folds of the $l_{in}^{th}$ repeat will be used to train a $q$-feature multivariate linear model (OLS). The model that yields the lowest validation RMSE will be recorded. 
3. After $k_{in}*l_{in}$ instances, there will fifty $q$-parameter (here, $q=3$) models. The model that occurs the most will be selected to train on the entire test-validation set to generate coefficients, RMSE, adjusted $R^2$, and other metrics. 
4. This final model will be used to conduct predictions on the test set. 

```{r in_var}
k.in<-5
l.in<-10
n.in<-k.in*l.in
```

## Helper Functions

We'll also define some helper functions. 

`get_model_formula()` will access the formula of models returned by `regsubsets()`, a function in the `leaps` package that will do exhaustive subset selection for feature selection. It takes the following arguments: 
* `id`: which row of the `regsubsets` output to access
    + `regsubsets()` outputs the feature results starting with 1-feature models, however many are specified by the argument `nbest`, all the way to `nvmax`. Accordingly, `regsubsets()` will return `nvmax*nbest` results. 
    + We are interested in looking at a $q$-parameter model, so it makes sense to examine the rows $(q-1)*nbest+1$ to $q*nbest
* `object`: output of `regsubsets()`
* `dv`: a character string of the dependent variable (here, `dv`="ddG)

```{r get_model_formula}
get_model_formula<-function(id, object,dv="ddG") {
      #save logical matrix of features from regsubsets() and subset the id of interest 
      # -1 removes intercept column
       models<-summary(object)$which[id,-1]
      # Preserve only the features that are used 
      predictors<-names(which(models==T))
        # Concatenate features into a string separated by + 
        predictors<-paste(predictors, collapse="+")
      # Concatenate features with ~ and outcome into character string 
      full<-paste0(dv, "~", predictors)
      }
```

Another helper function that will be used is `get_cv_rmse()`, which will use the formula outputted by `get_model_formula()` to train on the training set and predict validation set RMSE. 

* `formula_string`: output of `get_model_formula()`, a character vector 
* `train_set`: training set 
* `val_set`: validation set containing measured output values 
* `mean_list`: list of scaled means
* `sd_list`: list of scaled standard deviations
    + ddG RMSEs should be evaluated with the unscaled ddG (not the sacled z-score)
    + `i.out` refers to the $i^{th}$ training-validation and test set split of the outer loop
* `dv`: dependent variable, a character vector

```{r get_cv_rmse_f}
# Create function that will use formula to predict validation set and calculate rmse 

get_cv_rmse<-function(formula_string, train_set, val_set, 
                      mean_list=tv.mean.l, sd_list=tv.sd.l, dv="ddG", 
                      i.out){
  # define measured validation ddG as vector 
  val.meas<-val_set[, 
               which(colnames(val_set)==dv)]
  
  #convert string to formula
  formula<-as.formula(formula_string)
  
  # create model lm() for prediction
  object<-lm(formula, data=train_set)
  
  # predict validation ddG (scaled) given inputs of validation
  pd_cv<-predict(object, newdata=val_set)
  
  # find mean and sd ddG for scaling 
  mean.ddg<-mean_list[[i.out]][dv]
  sd.ddg<-sd_list[[i.out]][dv]
  
    # convert scaled ddG to original 
    pd_cv<-pd_cv*sd.ddg+mean.ddg
    
    # scale back ddG of validation set 
    val_usc<-val.meas*sd.ddg+mean.ddg
  
  # calculate RMSE 
  rmse_cv<-sqrt(mean((pd_cv-val_usc)^2))
  
  return(rmse_cv)}



```

## Executing CV Feature Seletion with `regsubsets()`

Execute repeated cross validation on the training-validation (tv) set. For each tv set,there are $n_{in}=k_{in}\times l_{in}$ training sets and an equal number of validation sets. 

`cv.ftsel.f()` is a function that will execute the repeated cross validation and output the $q$-feature model with the lowest validation RMSE for each training instance. 

* `i.in`: index of a given training and validation set split; $i_{in}=1, 2, ..., n_{in}$
* `tv`: tv set
* `l.in`: number of repeats to perform 
* `k.in`: number of folds to create for each repeat; note that folds within the same repeat do not overlap. 
* `h`: the number of best models (`nbest` argument in `regsubsets()`) to try for each training set 
    + From the `h` best models, the model with the lowest validation RMSE will be selected as the "champion" model for the fold
    + Best, as defined by `regsubsets()` is determined by RSS (residual square sums) of a given model in fitting the training set; it can be seen as a measure of bias 
* `seed`: a whole number to set the random number generator used by `createMultiFolds()`
* `fold.var` is a factor variable used as an input of `createMultiFolds()`
* `q`: number of parameters; here, since $n=29$, $q=3$


```{r cv_function, warning=F}

cv.ftsel.f<-function(i.in, tv, 
                     k.in=5, l.in=10, h=5, 
                     seed=1998, fold.var="L.Class", q=3, i.out){
  # Set seed
  set.seed(seed, sample.kind="Rejection")
  
  # Stored repeated cv indices in previously created list 
  folds.idx<-createMultiFolds(tv[,which(colnames(tv)==fold.var)], 
                             k.in, times=l.in)
  # Call fold by index above 
    fold_i<-folds.idx[[i.in]]
    
  # Subset training set 
    train<-tv[fold_i,]
    
   # Subset validation set 
    val<-tv[-fold_i,]
  
    # Subset selection of training set 
    # Remove labelers L.Class and ID 
    fmod<-regsubsets(ddG~., data=train[,-c(
      grep("ID", colnames(train)), 
      grep("L.Class", colnames(train))
    )], really.big=T,nvmax=q, 
    nbest=h, method="exhaustive")
    
     # RMSE of the h best q-feature models output by regsubsets()
      # Concatenate the h best q-feature model 
      #and extract model formula as character string 
      bestq.h<-sapply(((q-1)*h+1):(q*h), FUN=get_model_formula, object=fmod)
      
      # For each of the formulas, find RMSE of validation set 
        # sd and mean lists have already been defined
      bestq.h.rmse<-sapply(bestq.h[1:length(bestq.h)], FUN=get_cv_rmse, 
                           train_set=train, val_set=val, i.out=i.out)
      
       # concatenate into df 
      df.bestq.h<-data.frame("Model"=bestq.h, 
                        "Val.RMSE"=bestq.h.rmse)
        # remove row names 
        rownames(df.bestq.h)<-c()
        
      # Subset the row with the lowest validation rmse 
      df.bestq<-df.bestq.h[which(df.bestq.h$Val.RMSE==min(df.bestq.h$Val.RMSE)),]
      
      # Print this row 
      return(df.bestq)
}
```

We'll now use the above function within a larger function. 

## Nested CV 

`nested.cv.f()` takes a given tv set and and using `cv.ftsel.f()`, outputs two data frames: 

1. `features`: The output of passing the tv set through `cv.ftsel.f()` across all $n_{in}$ inner folds.Each row is the best $q$-feature model (i.e. lowest validation RMSE) for the $i_{in}^{th}$ inner fold. 
2. `features.summary`: A summary of `features`, showing the frequency of each unique model and its average validation RMSE. 
    + $\sum{Frequency}=n_{in}$ (Here, the sum of the frequencies is $n_{in}=k_{in} \times l_{in}=50$)
    + This table is sorted by decreasing frequencies; if there are ties in frequency, secondary sort by increasing average validation RMSE. 
    

```{r nested_cv}
nested.cv.f<-function(i.out, 
                      k.in=5, l.in=10, h.in=5, 
                      seed.in=1998, q=3){
  # call tv set from list 
  tv.i<-tv.sc.l[[i.out]]
  
  # pass this scaled tv set through cv.ftsel.f
  features.i<-t((sapply(1:n.in, FUN=cv.ftsel.f, 
       k.in=k.in, l.in=l.in, h=h.in, 
       seed=seed.in, q=q,
       tv=tv.i, i.out=i.out)))
  
  # convert sapply() output array to dataframe
  features.i<-data.frame(features.i)
  
    # both columns are currently lists, which needs to be converted to 
    # respective classes
      features.i$Model<-as.factor(unlist(features.i$Model))
      features.i$Val.RMSE<-as.numeric(unlist(features.i$Val.RMSE))
      
  # concatenate into a df that lists the number of votes each 
  # model receives along with its average RMSE 
          
  features.summary<-merge(
    #frequency that the model is recommended as best
    data.frame(table(features.i$Model)), 
            # Avg. validation rmse of given model
        data.frame("Avg Val RMSE"=tapply(features.i$Val.RMSE, INDEX=features.i$Model, 
       FUN=mean)), 
       by.x="Var1", 
       by.y=0)

  # Rename Var1 as Model 
  colnames(features.summary)[1]<-"Model"

  # Sort by descending counts 
  features.summary<-features.summary[
    order(-features.summary$Freq, 
          features.summary$Avg.Val.RMSE),
    ]

return( list(
  "features"=features.i, 
  "features.summary"=features.summary)
)
  
}

```

### Applying `nested.cv.f` Over Training-validation Set 

`nested.cv.f` will be applied across all training-validation sets to generate a list of length $n_{out}$.

```{r apply_nested_fun, warning=F}
# Pass nested.cv.f through all values of i.out 

ft.l<-lapply(1:n.out, 
              FUN=nested.cv.f)

# Example: first tv set 
kable(ft.l[[1]])
```


### Selecting TV-Specific Champion Model 

For each tv set, the "best" model can be considered as the model appearing with the highest frequency.

`feature_list`: list of feature summaries, i.e. `ft.l`

```{r best_model_sel}
best.modsel.f<-function(i.out,feature_list=ft.l){
  
  # call summary table by i.out of appropriate tv set
  f<-feature_list[[i.out]]$features.summary

  # convert model to character for sapply() later
  f$Model<-as.character(f$Model)
  
  # Because of how features.summary is sorted, the 
  # first row is considered the best model 
  return(f[1,])
  
    }

```
    
Apply `best.modsel.f()` over all the `n.out` tv folds. 

```{r best_model_sel_apply}
df.best.mod<-data.frame("i.out"=1:n.out,
    t(sapply(1:n.out, FUN=best.modsel.f, 
           ft.l)))

# The columns of df.best.mod are lists, which need to be 
# Converted into character or numerical vectors. 

for(i in 1:ncol(df.best.mod)){
  df.best.mod[,i]<-unlist(df.best.mod[,i])
}

# Revert model to factor variable 
df.best.mod$Model<-as.factor(df.best.mod$Model)

# Display best mod
kable(df.best.mod)
```

`df.best.mod` can be visualized.

The boxplot shows the distribution of each model's average validation RMSEs (i.e. calculated from validation RMSEs of the $n_{in}$ folds) across the $n_{out}$ tv sets. Number to the right of each boxplot is the frequency (out of $n_{out}$)

```{r best_mod_vis}
# Boxplot of Average Validation RMSEs
ggplot(df.best.mod, aes(x=Model, y=Avg.Val.RMSE, 
                        color=Model)) + 
  geom_boxplot() + 
  theme(legend.position="none") + 
  ylab("Average Validation RMSE")+ 
  ggtitle(paste(c("Average Validation RMSE of Best 
Models Across TV/Test (Outer) Splits,", 
                  coordination), collapse=" ")) + 
  geom_label(stat="count", 
            aes(label=..count..), 
            y=max(df.best.mod$Avg.Val.RMSE)+
              0.02) +
  ylim(min(df.best.mod$Avg.Val.RMSE), 
       max(df.best.mod$Avg.Val.RMSE)+0.05) +
  coord_flip()
  
```

# Training the Models

Define the function `train.f()` that will take a character vector of $n_{out}$ formulas and train them on the corresponding scaled tv set (`scaled_tvlist()`), returning a list of $n_{out}$ `lm()` objects for future prediction. 

```{r train_f}
train.f<-function(i.out, formula_vector, 
                  scaled_tvlist){
  #Subset ith formula 
  formula.i<-formula(formula_vector[i.out])
  
  # Subset ith tv set, which should be scaled
  tv<-scaled_tvlist[[i.out]]
  
  # Run lm()
  model<-lm(formula.i, 
            tv)
  
  return(model)
}
```

Apply `train.f()` across $n_{out}$ tv sets to create list of `lm()` objects. 

```{r train}
# Freq trained mods 
trmods.l<-lapply(
  1:n.out, 
  FUN=train.f, 
  formula_vector=as.character(df.best.mod$Model), 
  scaled_tvlist=tv.sc.l
)
```

`adjr2.f()` is a function that will output the adjusted $R^2$ from each of the trained models. 

```{r adjr2_f}
adjr2.f<-function(i.out, trainedmods_list=trmods.l){
  # Subset out model 
  mod<-trainedmods_list[[i.out]]
  
  # find adjr2
  return(
    summary(mod)$adj.r.squared)
}
```

Apply `adjr2.f()` across $n_{out}$ tv sets. 

```{r adjr2f_apply}
# Create df to show adr2
df.adjr2<-data.frame(
  "i.out"=1:n.out,
  "Model"=as.factor(df.best.mod$Model), 
  "adjr2"=sapply(1:n.out, FUN=adjr2.f)
)

# Summary table 
df.adjr2.summary<-data.frame(
  "Model"=levels(df.best.mod$Model),
  "Average AdjR2"=
  tapply(df.adjr2$adjr2, 
         df.adjr2$Model, 
         mean)
      )
  # Remove row names 
  rownames(df.adjr2.summary)<-c()

  
# Show summary 
kable(
  df.adjr2.summary[order(df.adjr2.summary$Average.AdjR2, 
                         decreasing=T),]
)

```

Visualize distribution of Adusted $R^2$ using boxplot. Number to the right of each boxplot is the frequency (out of $n_{out}$)

```{r adjr2_vis}
# Visualize 
ggplot(df.adjr2, 
       aes(x=Model, 
           color=Model)) + 
  geom_boxplot(aes(
    y=adjr2
  )) + 
  theme(legend.position = "none")+
  ggtitle(paste(
    c("Distribution of Adjusted R2 of Each Model, by Method,", coordination), collapse=" ")
  ) +
  geom_label(stat="count", 
            aes(label=..count..), 
            y=max(df.adjr2$adjr2)+0.02) + 
  ylim(min(df.adjr2$adjr2), 
       max(df.adjr2$adjr2)+0.05) + 
  coord_flip()
```


# Testing the Models 

## Scaling the Test Set 

`scale.ts.f()` scales each test set according to the scaling attributes (mean, standard deviation) of the corresponding tv set. 

* `test_list`: the list of unscaled test sets
* `center_list`: the list of means
* `scale_list`: the list of standard deviations

```{r scale_ts_f}
scale.ts.f<-function(i.out, 
                     test_list=ts.l, 
                     center_list=tv.mean.l,
                     scale_list=tv.sd.l){
  # Call test set 
  test<-test_list[[i.out]]
  
  # Remove ID, L.Class 
  test.num<-test[,-c(
    grep("ID", colnames(test)), 
    grep("L.Class", colnames(test))
  )]
  
  # Call center from list 
  center.i<-center_list[[i.out]]

  # Call scale from lsit 
  scale.i<-scale_list[[i.out]]
  
  # scale test.num
  test.sc0<-scale(test.num, 
                  center=center.i, 
                  scale=scale.i)
  
  # convert to df, add back ID and L.Class
  test.sc<-data.frame(
    "ID"=test$ID, 
    as.data.frame(test.sc0), 
    "L.Class"=test$L.Class
  )
  
  # return 
  return(test.sc)
}

#Apply function over all test sets  
ts.sc.l<-lapply(1:n.out, 
       FUN=scale.ts.f)
```


## Predictions 

Define the function `predict.f` that, for a given tv/test (outer) split, will predict the ΔΔG^‡^ of the training-test set as well as the test set (values stored as a vector). The function will then rescale these predicted values and make a dataframe displaying predictions against measured ddGs. 

* `trained_mods_list`: list of `lm()` objects; length $n_{out}$
* `dv`: a string of the dependent variable 

Output data frame: 
* `Predicted`: predicted ΔΔG^‡^;unscaled 
* `Measured`: measured ΔΔG^‡^; unscaled 
* `Set`: factor variable of either Training-Validation or test 
* `ID`: ligand ID 

```{r predict_f}
predict.f<-function(i.out, trained_mods_list=trmods.l, 
                    scaled_tv_list=tv.sc.l,scaled_test_list=ts.sc.l, 
                    unscaled_tv_list=tv.l, unscaled_test_list=ts.l,
                    center_list=tv.mean.l, scale_list=tv.sd.l, dv="ddG"){
  # Call ith model 
  mod<-trained_mods_list[[i.out]]
  
  # Call ith scaled tv set 
  tv.sc<-scaled_tv_list[[i.out]]
  
  # Call ith scaled test set 
  test.sc<-scaled_test_list[[i.out]]
  
  # Call mean ddG value of ith tv set 
  center.i<-center_list[[i.out]][dv]
  
  # Pull sd ddG value of ith tv set 
  scale.i<-scale_list[[i.out]][dv]
  
  # Predict tv ddG 
  pred.tv.sc<-predict(mod, 
                      newdata=tv.sc)
  
  # Predict test ddG 
  pred.ts.sc<-predict(mod, 
                      newdata=test.sc)
  
  # Rescale tv ddG predictions 
  pred.tv<-pred.tv.sc*scale.i+center.i

  # Rescale tv ddG predictions 
  pred.ts<-pred.ts.sc*scale.i+center.i

  # Concatenate into df 
  df.pred<-data.frame(
    "ID"=c(tv.sc$ID, 
           test.sc$ID),
    'Predicted'=c(pred.tv, 
                  pred.ts),
    'Measured'=c(unscaled_tv_list[[i.out]][, 
                                           which(colnames(unscaled_tv_list[[i.out]])
                                                 ==dv)], 
                 unscaled_test_list[[i.out]][, 
                                           which(colnames(unscaled_test_list[[i.out]])
                                                 ==dv)]), 
    "Set"=c(
      rep("Training-Validation", length(pred.tv)),
      rep("Test", length(pred.ts),)
    )
  )
  
  return(df.pred)
                    }
  
```

Apply `predict.f()` to all $n_{out}$ tv and test sets, outputting a list of $n_{out}$ data frames.  

```{r apply_pred_f}
pred.l<-lapply(1:n.out, 
                    FUN=predict.f)
```

### Finding RMSE 
Define a function `rmse.f` that will find the training-validation and test RMSEs. 

```{r rmse_f}
rmse.f<-function(i.out, pred_list){
  
  # Call ith predicted/measured df 
  pred.i<-pred_list[[i.out]]
  
  # Subset training/validation RMSE
    tv<-pred.i[pred.i$Set=="Training-Validation",]
        # Find tv RMSE 
        tv.rmse<-sqrt(
          mean(
            (tv$Predicted-tv$Measured)^2
          )
        )
        
    # Subset test RMSE 
    ts<-pred.i[pred.i$Set=="Test",]
        # Find test RMSE 
        ts.rmse<-sqrt(
          mean(
            (ts$Predicted-ts$Measured)^2
          )
        )
        
    # Create df
      
      df.rmse<-data.frame(
        "i.out"=i.out, 
        "TV RMSE"=tv.rmse,
        "Test RMSE"=ts.rmse
      )
      
      return(df.rmse)
      
}
```

Apply this function over the outer folds. 

```{r rmse_f_apply}
df.rmse<-data.frame(
"Model"=df.best.mod$Model,
  t(
      sapply(1:n.out, 
         FUN=rmse.f,
         pred.l))
)

#Unlist each column 
for(i in 1:ncol(df.rmse)){
  df.rmse[,i]<-unlist(df.rmse[,i])
}

kable(head(df.rmse))
```

Create summary table showing the frequency of the models across the outer folds and the corresponding average training-validation and test RMSEs. This table will be sorted by descending frequency. If there is a tie in frequency, secondary sort by ascending average test RMSE. 

```{r rmse_f_apply}
# Find average RMSE for each model
# Models chosen by popularity 
df.rmse.summary<-data.frame(
  Model=levels(
    as.factor(
          df.rmse$Model
  )), 
  "Average TV RMSE"=as.numeric(tapply(df.rmse$TV.RMSE, 
       INDEX=df.rmse$Model, 
       FUN=mean)), 
  "Average Test RMSE"=as.numeric(tapply(df.rmse$Test.RMSE, 
       INDEX=df.rmse$Model, 
       FUN=mean)), 
  'Adjusted R2'=as.numeric(tapply(df.adjr2$adjr2, 
                                  INDEX=df.adjr2$Model, 
                                  FUN=mean)),
  "Frequency"=as.numeric(
    table(df.rmse$Model)
  )
)

# Reorder 
df.rmse.summary<-df.rmse.summary[
  order(-df.rmse.summary$Frequency, 
        df.rmse.summary$Average.Test.RMSE), 
]
  
#Display summary table 
kable(df.rmse.summary)


```

## Visualizing RMSE

From `df.rmse`, a plot of Test RMSE vs. TV RMSE can be constructed for each tv-test (outer) split. The points will be colored by the combination of features used, and the dashed line represents $y=x$. 

```{r test_vs_tv_rmse_vis}
# Scatterplot of TV/Test RMSEs, by Model and Method 
ggplot(df.rmse)+ 
  geom_point(aes(
    x=TV.RMSE, 
    y=Test.RMSE, 
    color=Model
  ), 
  alpha=0.6) + 
  ggtitle(paste(
    c("Training-Validation and Test RMSE,", 
      coordination), collapse=" ")
    )+ 
  geom_segment(aes(x=0.1, 
                   y=0.1,
                  xend=max(TV.RMSE), 
                   yend=max(TV.RMSE)), 
               linetype="dashed", 
               alpha=0.6/n.out)
```

Subsequently, a box plot of RMSE distributions for each model can be constructed. The facet grids show the tv and test RMSE sets. 

```{r rmse_boxplot}
# Melt data set into wide format for ggplot
df.rmse.m<-melt(df.rmse, 
id.vars=c("i.out", 
          "Model")
)
# Rename column 
colnames(df.rmse.m)[3]<-"Set"

# PLot
ggplot(df.rmse.m, 
        aes(x=Model, 
        y=value, 
        color=Model)) + 
  geom_boxplot() + 
  facet_grid(rows=vars(Set),
             scales="free") + 
  theme(axis.text.x=element_text(angle=-45, 
                                 hjust=0, 
                                 vjust=1), 
        legend.position="none", 
        plot.margin = unit(c(1,2,0,1), "cm")) + 
  ylab("RMSE") + 
  ggtitle(paste0(c("TV and Test RMSE Distributions for Each Model", 
                   coordination), collapse=" "
  )) 
```

## Visualizing Predictions 

Define a function `ggplot.f()` that will plot the measured against the predicted ddGs. 

* `pred_list`: list of data frames that show predicted against measured outputs (e.g. `pred.l`)
* `mod_vector`: a character vector of model names (e.g. `df.best.mod$Model`)
* `rmse_df`: a dataframe containing RMSEs (e.g. `df.rmse`)
* `adjr2_df`: a dataframe containing adjusted $R^2$ (e.g. `df.adjr2`)
* `display_id`: a logical argument; when set to `TRUE`, ID labels of individual observations will be displayed

```{r ggplot_f}
ggplot.f<-function(i.out, pred_list=pred.l, 
                   mod_vector=df.best.mod$Model, rmse_df=df.rmse, 
                   adjr2_df=df.adjr2, display_id=F){ 
  # Call prediction df 
  pred<-pred_list[[i.out]]
  
  # Call model name 
  mod.name<-as.character(mod_vector[i.out])
  
  # Call tv and test rmse 
    tv.rmse<-round(
      rmse_df[i.out, 
      which(colnames(rmse_df)=="TV.RMSE")], 3)
    
    test.rmse<-round(
      rmse_df[i.out, 
      which(colnames(rmse_df)=="Test.RMSE")], 3)
    
  # Call corresponding adjr2 info 
       adjr2<-round(
             adjr2_df[i.out, 
                      which(colnames(adjr2_df)=="adjr2")], 3)
       
  # ggplot 
 plot.i<-ggplot(pred, 
         aes(x=Measured, 
             y=Predicted, 
             color=Set)) + 
    geom_point(alpha=0.6)+
    geom_segment(aes(x=min(Measured),xend=max(Predicted),
                     y=min(Measured),yend=max(Predicted)), 
               linetype="dashed", alpha=0.6/n.out) + 
  ggtitle(paste(
    c(
      "PARC Epoxide (n=29),", coordination, ",", "TV-Test Split Index=", 
      i.out), collapse=" ")
    )+ 
  xlab("Measured ΔΔG (kcal/mol)") + 
  ylab("Predicted ΔΔG (kcal/mol)") + 
  annotate("text", 
           label=paste(mod.name), 
           y=0.4, 
           x=1
           ) + 
   annotate("text", 
           label=paste(c(
             "Adjusted R2=", adjr2),
             collapse=" "), 
           y=0.3, 
           x=1
           ) + 
   annotate("text", 
           label=paste(
             c("Training-Validation RMSE=", 
               tv.rmse), 
             collapse=" "), 
           y=0.2, 
           x=1
           ) + 
   annotate("text", 
           label=paste(
             c("Test RMSE=", 
               test.rmse), 
             collapse=" "), 
           y=0.1, 
           x=1
           ) 
 
# Toggle for displaying individual IDs
ifelse(display_id==T, 
       return(plot.i+ geom_text(aes(label=ID), 
             hjust=0,
             vjust=-1)), 
       return(plot.i))
 
  }
```

Apply `ggplot.f()` across all of the outer folds and store the results in a list. Use the `gridExtra` package to arrange these plots into a larger tile. Because there are $n_{out}$ plots, the tiled panels may need to be saved as a large image for viewing. 

```{r ggplot_f_use, eval=F}
# Apply ggplot.f()
ggplot.pred<-lapply(1:n.out, 
       FUN=ggplot.f)

  # Check 
  ggplot.pred[[1]]
  
# Arrange 
grid.arrange(grobs=ggplot.pred)

```

# Comparing Models Across Coordination States: 5x2 CV

Take champion model (defined as model appearing most frequently out of $n_{out}$) for each of these coordination states, train them over a 5x2 (5 repeats, 2 folds) cross-validation scheme, and predict the test RMSE.

We'll first create our folds and scale them. 
```{r cf_folds}
# Set seed
set.seed(1998, sample.kind = "Rejection")

# Create fold index for 5x2 CV
cf_fold_idx<-createMultiFolds(df$L.Class, 
                           k=2, times=5)
  # Define total number of folds (5x2)
  n_cf<-10

# Create folds 
  # Train
cf.train.l<-lapply(1:n_cf,
  FUN=split.out.f, fold_idx=cf_fold_idx, 
  df0=df, tv=T)
  
  # Test 
cf.test.l<-lapply(1:n_cf,
  FUN=split.out.f, fold_idx=cf_fold_idx, 
  df0=df, tv=F)
```

Scale training folds, saving the scaling attributes. Scale test folds according to the saved scaling attributes.  

```{r cf_scale}
# Scale training set 
cf.tr.sc<-lapply(1:n_cf, 
                FUN=scale.f, 
                tv_list=cf.train.l, 
                dfs=T)

  # Save scaling attributes 
  cf.tr.sc.mean<-lapply(1:n_cf, 
                FUN=scale.f, 
                tv_list=cf.train.l, 
                center=T)
  
  cf.tr.sc.sd<-lapply(1:n_cf, 
                FUN=scale.f, 
                tv_list=cf.train.l, 
                scale=T)
  
# Scale test set according to training set scaling attributes 
  cf.ts.sc<-lapply(1:n_cf, 
       FUN=scale.ts.f, 
       cf.test.l, 
       cf.tr.sc.mean,
       cf.tr.sc.sd)
```

Train champion model all ten of these training sets and predict the test RMSEs.


```{r cf_train}
# define champion model 
champ<-as.formula(
  df.rmse.summary[1,
                   which(colnames(df.rmse.summary)=="Model")]
)

# Train
cf.trmods<-list()
  
for(i in 1:n_cf){
  cf.trmods[[i]]<-lm(champ, 
                     data=cf.tr.sc[[i]])
}

# find adjusted R2 
cf.adjr2<-sapply(1:n_cf, FUN=adjr2.f, 
       cf.trmods)
```

Evaluate on test set and find RMSE
```{r cf_predict}
# predict on test set 
cf.pred<-lapply(1:n_cf, 
                    FUN=predict.f, 
                    trained_mods_list=cf.trmods,
                    scaled_tv_list=cf.tr.sc,scaled_test_list=cf.ts.sc, 
                    unscaled_tv_list=cf.train.l, unscaled_test_list=cf.test.l,
                    center_list=cf.tr.sc.mean, scale_list=cf.tr.sc.sd, dv="ddG")

# find RMSE
cf.rmse<-as.data.frame(t(sapply(1:n_cf, 
             FUN=rmse.f, 
             cf.pred)))
  
  # rename rmse column with coordination 
  for(i in 1:ncol(cf.rmse)){
    colnames(cf.rmse)[i]<-paste(
      c(colnames(cf.rmse)[i], 
        coordination), collapse=" "
    )
  }
  
#Display
  kable(cf.rmse)
```
